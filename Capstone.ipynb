{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4799  | total loss: \u001b[1m\u001b[32m0.25431\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 200 | loss: 0.25431 - acc: 0.9471 -- iter: 184/189\n",
      "Training Step: 4800  | total loss: \u001b[1m\u001b[32m0.25434\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 200 | loss: 0.25434 - acc: 0.9324 -- iter: 189/189\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\nitin\\Python\\Capstone\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "with open(\"intents_Sud.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            #print(\"with tokenize:\\n\", wrds)\n",
    "            wrds = [wrd for wrd in wrds if wrd.lower() not in stopwords]\n",
    "            #print(\"without stopwords:\\n\", wrds)\n",
    "            wrds = [wrd.lower() for wrd in wrds if wrd not in string.punctuation]\n",
    "            #print(\"without punctuation:\\n\", wrds)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [lemmatizer.lemmatize(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "#try:\n",
    "#model.load('./model.tflearn')\n",
    "#except:\n",
    "model.fit(training, output, n_epoch=200, batch_size=8, show_metric=True)\n",
    "model.save('./model.tflearn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)!\n",
      "You: hi\n",
      "Bot:  Good to see you visit us.\n",
      "You: hello\n",
      "Bot:  Hi there, how can I help?\n",
      "You: how to pay?\n",
      "Bot:  COD is not available for now, we accept only online payments.\n",
      "You: what is the address to visit?\n",
      "Bot:  You can visit our shop at below address: \\n V.L Mashere, shop no 4, halailohana mahajan trust building, chendani naka, Station Rd, next to patel saree center, near ashok cinema, Thane West, Thane, Maharashtra 400601\n",
      "You: what are types of agarbattis you have?\n",
      "Bot:  We have wide variety of puja related products and agarbattis.\n",
      "You: which scented agarbattis you have?\n",
      "Bot:  COD is not available for now, we accept only online payments.\n",
      "You: do you have masala agarbattis?\n",
      "Bot:  The costs of Wet Dhoop Sticks are as follows:Gugal-20pcs-35Rs,  Loban-20pcs-35Rs , Champa-20pcs-35Rs, Gulab-20pcs-35Rs, Mogra-20pcs-35Rs and Prachin-20pcs-35Rs. It will be packed in Zipper polybags.\n",
      "You: premium masala\n",
      "Bot:  We have Masala Agarbatti in different types like Mantra, Flora, Nag Champa, Nitya Puja, Masala Mogra, Masala Gulab, Indrayani Masala, Azaroo Masala, Natural Masala, Chandan Masala, Sai Flora Masala, Gold Flora, Chintamani Flora, Maratha Flora, Samrat Nag Champa, Loban Masala, Sai Chafa, White Gold, Azaroo and Natural.\n",
      "You: you have pouch?\n",
      "Bot:  Good to see you visit us.\n",
      "You: agarbatis in pouch\n",
      "Bot:  Hi there, how can I help?\n",
      "You: nag champa?\n",
      "Bot:  Masala Agarbattis will be box packed.\n",
      "You: flora\n",
      "Bot:  See you later, thanks for visiting.\n",
      "You: how will you deliver?\n",
      "Bot:  Hi there, how can I help?\n",
      "You: is your product packed?\n",
      "Bot:  The product will be box packed.\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [lemmatizer.lemmatize(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "        #print(results)\n",
    "        #print(tag)\n",
    "\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "            \n",
    "        print(\"Bot: \",random.choice(responses))\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DhoopPowder',\n",
       " 'DhoopSticks',\n",
       " 'DryDhoopSticks',\n",
       " 'MasalaAgarbatti',\n",
       " 'PremiumMasalaAgarbatti',\n",
       " 'PremiumScentedAgarbatti',\n",
       " 'ScentedAgarbatti',\n",
       " 'WetDhoopSticks',\n",
       " 'address',\n",
       " 'goodbye',\n",
       " 'greeting',\n",
       " 'hours',\n",
       " 'opentoday',\n",
       " 'payments',\n",
       " 'product',\n",
       " 'quantity',\n",
       " 'sell',\n",
       " 'thanks']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " '50gm',\n",
       " 'accept',\n",
       " 'address',\n",
       " 'agar',\n",
       " 'agarbatti',\n",
       " 'agarbattis',\n",
       " 'akash',\n",
       " 'akkalkot',\n",
       " 'alphanso',\n",
       " 'amchi',\n",
       " 'anyone',\n",
       " 'available',\n",
       " 'azaroo',\n",
       " 'blue',\n",
       " 'box',\n",
       " 'brut',\n",
       " 'buy',\n",
       " 'bye',\n",
       " 'c',\n",
       " 'card',\n",
       " 'cash',\n",
       " 'chafa',\n",
       " 'champa',\n",
       " 'chandan',\n",
       " 'chintamani',\n",
       " 'cod',\n",
       " 'confidence',\n",
       " 'cost',\n",
       " 'credit',\n",
       " 'dasang',\n",
       " 'day',\n",
       " 'delivery',\n",
       " 'dhoop',\n",
       " 'di',\n",
       " 'different',\n",
       " 'done',\n",
       " 'dry',\n",
       " 'english',\n",
       " 'firaki',\n",
       " 'flora',\n",
       " 'flower',\n",
       " 'get',\n",
       " 'gold',\n",
       " 'golden',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'gugal',\n",
       " 'gulab',\n",
       " 'heena',\n",
       " 'hello',\n",
       " 'helpful',\n",
       " 'helping',\n",
       " 'hi',\n",
       " 'holy',\n",
       " 'hour',\n",
       " 'incense',\n",
       " 'indrayani',\n",
       " 'information',\n",
       " 'jai',\n",
       " 'jewel',\n",
       " 'jordan',\n",
       " 'kashmiri',\n",
       " 'kasturi',\n",
       " 'kesari',\n",
       " 'king',\n",
       " 'later',\n",
       " 'lavender',\n",
       " 'litchi',\n",
       " 'loban',\n",
       " 'lot',\n",
       " 'magnum',\n",
       " 'mallika',\n",
       " 'mango',\n",
       " 'mantra',\n",
       " 'many',\n",
       " 'maratha',\n",
       " 'masala',\n",
       " 'mastercard',\n",
       " 'mata',\n",
       " 'milan',\n",
       " 'mogra',\n",
       " 'much',\n",
       " 'mumbai',\n",
       " 'musk',\n",
       " 'nabil',\n",
       " 'nag',\n",
       " 'namaste',\n",
       " 'narayan',\n",
       " 'natural',\n",
       " 'nida',\n",
       " 'nitya',\n",
       " 'one',\n",
       " 'open',\n",
       " 'oudh',\n",
       " 'pack',\n",
       " 'packed',\n",
       " 'packing',\n",
       " 'pay',\n",
       " 'payment',\n",
       " 'pc',\n",
       " 'phool',\n",
       " 'pineapple',\n",
       " 'pisces',\n",
       " 'powder',\n",
       " 'prachin',\n",
       " 'premium',\n",
       " 'price',\n",
       " 'product',\n",
       " 'puja',\n",
       " 'purple',\n",
       " 'qty',\n",
       " 'quantity',\n",
       " 'ratrani',\n",
       " 'reach',\n",
       " 'rose',\n",
       " 'roselin',\n",
       " 'saffron',\n",
       " 'sai',\n",
       " 'sale',\n",
       " 'samrani',\n",
       " 'samrat',\n",
       " 'sandal',\n",
       " 'scented',\n",
       " 'see',\n",
       " 'sell',\n",
       " 'selling',\n",
       " 'shalimar',\n",
       " 'shop',\n",
       " 'shree',\n",
       " 'shreeniwasa',\n",
       " 'sold',\n",
       " 'special',\n",
       " 'stick',\n",
       " 'strawberry',\n",
       " 'sugandh',\n",
       " 'super',\n",
       " 'swami',\n",
       " 'swarna',\n",
       " 'take',\n",
       " 'tata',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'tirupati',\n",
       " 'today',\n",
       " 'type',\n",
       " 'u',\n",
       " 'vaishnavi',\n",
       " 'vaishnodevi',\n",
       " 'vanilla',\n",
       " 'variety',\n",
       " 'vignaharta',\n",
       " 'visit',\n",
       " 'vitthal',\n",
       " 'want',\n",
       " 'wassup',\n",
       " 'way',\n",
       " 'wet',\n",
       " 'white']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi'],\n",
       " [],\n",
       " ['anyone'],\n",
       " ['hello'],\n",
       " ['good', 'day'],\n",
       " ['wassup'],\n",
       " ['namaste'],\n",
       " ['bye'],\n",
       " ['see', 'later'],\n",
       " ['goodbye'],\n",
       " ['c', 'u', 'later'],\n",
       " ['tata'],\n",
       " ['bye'],\n",
       " ['thanks', 'lot'],\n",
       " ['thank'],\n",
       " [\"'s\", 'helpful'],\n",
       " ['thanks', 'information'],\n",
       " ['thank', 'helping'],\n",
       " ['hours', 'open'],\n",
       " ['hours'],\n",
       " ['open'],\n",
       " ['shop', 'open'],\n",
       " ['pay'],\n",
       " ['payments', 'done'],\n",
       " ['ways', 'payment'],\n",
       " ['accept', 'payment'],\n",
       " ['take', 'credit', 'cards'],\n",
       " ['accept', 'mastercard'],\n",
       " ['cash'],\n",
       " ['accept', 'cod'],\n",
       " ['cash', 'delivery'],\n",
       " ['open', 'today'],\n",
       " ['open', 'today'],\n",
       " ['hours', 'today'],\n",
       " ['sell'],\n",
       " ['u', 'sale'],\n",
       " ['selling'],\n",
       " ['available'],\n",
       " ['products', 'sold'],\n",
       " ['agarbattis'],\n",
       " ['sell', 'agarbattis'],\n",
       " ['agarbattis', 'available'],\n",
       " ['want', 'buy', 'agarbattis'],\n",
       " ['want', 'buy', 'incense', 'sticks'],\n",
       " ['shop', 'address'],\n",
       " ['shop'],\n",
       " ['shop'],\n",
       " ['visit', 'shop'],\n",
       " ['want', 'shop', 'address'],\n",
       " ['want', 'visit', 'shop'],\n",
       " ['visit', 'shop'],\n",
       " ['reach'],\n",
       " ['product'],\n",
       " ['product', 'box', 'packed'],\n",
       " ['different', 'types', 'products', 'u'],\n",
       " ['different', 'types', 'dhoop'],\n",
       " ['different', 'types', 'dhoop', 'u'],\n",
       " ['different', 'varieties', 'dhoop', 'u'],\n",
       " ['different', 'products'],\n",
       " ['quantity', 'products'],\n",
       " ['different', 'quantities', 'packed'],\n",
       " ['quantity', 'product'],\n",
       " ['quantity', 'get', 'agarbattis'],\n",
       " ['many', 'pisces', 'one', 'pack', 'agarbatti'],\n",
       " ['many', 'pcs', 'one', 'pack', 'agarbatti'],\n",
       " ['qty', 'products'],\n",
       " ['quantity', 'products'],\n",
       " ['different', 'types', 'dhoop', 'powder'],\n",
       " ['different', 'types', 'dhoop', 'powder'],\n",
       " ['packing', 'dhoop', 'powder'],\n",
       " ['much', '50gm', 'dhoop', 'powder', 'cost'],\n",
       " ['50gm', 'dhoop', 'powder', 'cost'],\n",
       " ['price', 'dhoop', 'powder'],\n",
       " ['cost', 'mallika'],\n",
       " ['cost', 'dasang'],\n",
       " ['different', 'types', 'dhoop', 'sticks'],\n",
       " ['different', 'types', 'dhoop', 'sticks'],\n",
       " ['packing', 'dhoop', 'sticks'],\n",
       " ['price', 'dhoop', 'sticks'],\n",
       " ['cost', 'golden', 'saffron'],\n",
       " ['cost', 'roselin'],\n",
       " ['cost', 'sai', 'flora'],\n",
       " ['cost', 'mango'],\n",
       " ['cost', 'vaishnodevi'],\n",
       " ['cost', 'loban'],\n",
       " ['cost', 'loban', 'samrani'],\n",
       " ['cost', 'oudh'],\n",
       " ['cost', 'gugal'],\n",
       " ['different', 'types', 'dry', 'dhoop', 'sticks'],\n",
       " ['different', 'types', 'dry', 'dhoop', 'sticks'],\n",
       " ['price', 'dry', 'dhoop', 'sticks'],\n",
       " ['cost', 'purple', 'jewel'],\n",
       " ['cost', 'blue', 'jewel'],\n",
       " ['different', 'types', 'masala', 'agarbatti'],\n",
       " ['different', 'types', 'masala', 'agarbatti'],\n",
       " ['price', 'masala', 'agarbatti'],\n",
       " ['cost', 'mantra'],\n",
       " ['cost', 'flora'],\n",
       " ['cost', 'nag', 'champa'],\n",
       " ['cost', 'nitya', 'puja'],\n",
       " ['cost', 'masala', 'mogra'],\n",
       " ['cost', 'masala', 'gulab'],\n",
       " ['cost', 'indrayani', 'masala'],\n",
       " ['cost', 'azaroo', 'masala'],\n",
       " ['cost', 'natural', 'masala'],\n",
       " ['cost', 'chandan', 'masala'],\n",
       " ['cost', 'sai', 'flora', 'masala'],\n",
       " ['cost', 'gold', 'flora'],\n",
       " ['cost', 'chintamani', 'flora'],\n",
       " ['cost', 'maratha', 'flora'],\n",
       " ['cost', 'samrat', 'nag', 'champa'],\n",
       " ['cost', 'loban', 'masala'],\n",
       " ['cost', 'sai', 'chafa'],\n",
       " ['cost', 'white', 'gold'],\n",
       " ['cost', 'azaroo'],\n",
       " ['cost', 'natural'],\n",
       " ['different', 'types', 'premium', 'masala', 'agarbatti'],\n",
       " ['different', 'types', 'premium', 'masala', 'agarbatti'],\n",
       " ['premium'],\n",
       " ['price', 'premium', 'masala', 'agarbatti'],\n",
       " ['cost', 'super', 'gold', 'sandal'],\n",
       " ['cost', 'heena', 'masala'],\n",
       " ['cost', 'nabil', 'masala'],\n",
       " ['cost', 'agar', 'oudh'],\n",
       " ['cost', 'sai', 'special'],\n",
       " ['cost', 'vaishnavi', 'flora'],\n",
       " ['cost', 'vitthal', 'flora'],\n",
       " ['cost', 'ratrani', 'masala'],\n",
       " ['cost', 'mogra', 'masala'],\n",
       " ['cost', 'sandal', 'king'],\n",
       " ['cost', 'white', 'rose'],\n",
       " ['cost', 'narayan', 'flora'],\n",
       " ['cost', 'vignaharta', 'flora'],\n",
       " ['cost', 'musk', 'heena'],\n",
       " ['cost', 'nida', 'flora'],\n",
       " ['different', 'types', 'premium', 'scented', 'agarbatti'],\n",
       " ['different', 'types', 'premium', 'scented', 'agarbatti'],\n",
       " ['price', 'premium', 'scented', 'agarbatti'],\n",
       " ['cost', 'kesari', 'chandan'],\n",
       " ['cost', 'golden', 'mogra'],\n",
       " ['cost', 'golden', 'gulab'],\n",
       " ['cost', 'ratrani'],\n",
       " ['cost', 'lavender'],\n",
       " ['cost', 'shree', 'akkalkot', 'swami'],\n",
       " ['cost', 'jai', 'mata', 'di'],\n",
       " ['cost', 'confidence'],\n",
       " ['cost', 'shalimar', 'sugandh'],\n",
       " ['cost', 'kashmiri', 'rose'],\n",
       " ['cost', 'magnum'],\n",
       " ['cost', 'tirupati'],\n",
       " ['cost', 'jordan'],\n",
       " ['cost', 'english', 'brut'],\n",
       " ['different', 'types', 'scented', 'agarbatti'],\n",
       " ['different', 'types', 'scented', 'agarbatti'],\n",
       " ['price', 'scented', 'agarbatti'],\n",
       " ['cost', 'musk'],\n",
       " ['cost', 'rose'],\n",
       " ['cost', 'chandan'],\n",
       " ['cost', 'vanilla'],\n",
       " ['cost', 'holy', 'stick'],\n",
       " ['cost', 'heena'],\n",
       " ['cost', 'mogra'],\n",
       " ['cost', 'akash', 'mogra'],\n",
       " ['cost', 'sandal'],\n",
       " ['cost', 'pineapple'],\n",
       " ['cost', 'litchi'],\n",
       " ['cost', 'alphanso'],\n",
       " ['cost', 'strawberry'],\n",
       " ['cost', 'shreeniwasa'],\n",
       " ['cost', 'white', 'flower'],\n",
       " ['cost', 'swarna', 'champa'],\n",
       " ['cost', 'chandan'],\n",
       " ['cost', 'kasturi'],\n",
       " ['cost', 'lavender'],\n",
       " ['cost', 'akash', 'phool'],\n",
       " ['cost', 'mallika'],\n",
       " ['cost', 'gulab'],\n",
       " ['cost', 'milan'],\n",
       " ['cost', 'firaki'],\n",
       " ['cost', 'amchi', 'mumbai'],\n",
       " ['different', 'types', 'wet', 'dhoop', 'sticks'],\n",
       " ['different', 'types', 'wet', 'dhoop', 'sticks'],\n",
       " ['price', 'wet', 'dhoop', 'sticks'],\n",
       " ['cost', 'gugal'],\n",
       " ['cost', 'loban'],\n",
       " ['cost', 'champa'],\n",
       " ['cost', 'gulab'],\n",
       " ['cost', 'mogra'],\n",
       " ['cost', 'prachin']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greeting',\n",
       " 'greeting',\n",
       " 'greeting',\n",
       " 'greeting',\n",
       " 'greeting',\n",
       " 'greeting',\n",
       " 'greeting',\n",
       " 'goodbye',\n",
       " 'goodbye',\n",
       " 'goodbye',\n",
       " 'goodbye',\n",
       " 'goodbye',\n",
       " 'goodbye',\n",
       " 'thanks',\n",
       " 'thanks',\n",
       " 'thanks',\n",
       " 'thanks',\n",
       " 'thanks',\n",
       " 'hours',\n",
       " 'hours',\n",
       " 'hours',\n",
       " 'hours',\n",
       " 'payments',\n",
       " 'payments',\n",
       " 'payments',\n",
       " 'payments',\n",
       " 'payments',\n",
       " 'payments',\n",
       " 'payments',\n",
       " 'payments',\n",
       " 'payments',\n",
       " 'opentoday',\n",
       " 'opentoday',\n",
       " 'opentoday',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'sell',\n",
       " 'address',\n",
       " 'address',\n",
       " 'address']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
