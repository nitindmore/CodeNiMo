{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4799  | total loss: \u001b[1m\u001b[32m0.25431\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 200 | loss: 0.25431 - acc: 0.9471 -- iter: 184/189\n",
      "Training Step: 4800  | total loss: \u001b[1m\u001b[32m0.25434\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 200 | loss: 0.25434 - acc: 0.9324 -- iter: 189/189\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\nitin\\Python\\Capstone\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tflearn\n",
    "#import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('WordNet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            #print(\"with tokenize:\\n\", wrds)\n",
    "            wrds = [wrd for wrd in wrds if wrd.lower() not in stopwords]\n",
    "            #print(\"without stopwords:\\n\", wrds)\n",
    "            wrds = [wrd.lower() for wrd in wrds if wrd not in string.punctuation]\n",
    "            #print(\"without punctuation:\\n\", wrds)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [lemmatizer.lemmatize(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "#tf.compat.v1.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "#try:\n",
    "#model.load('./model.tflearn')\n",
    "#except:\n",
    "model.fit(training, output, n_epoch=200, batch_size=8, show_metric=True)\n",
    "model.save('./model.tflearn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking with the bot (type quit to stop)!\n",
      "You: hi\n",
      "Bot:  Good to see you visit us.\n",
      "You: hello\n",
      "Bot:  Hi there, how can I help?\n",
      "You: how to pay?\n",
      "Bot:  COD is not available for now, we accept only online payments.\n",
      "You: what is the address to visit?\n",
      "Bot:  You can visit our shop at below address: \\n V.L Mashere, shop no 4, halailohana mahajan trust building, chendani naka, Station Rd, next to patel saree center, near ashok cinema, Thane West, Thane, Maharashtra 400601\n",
      "You: what are types of agarbattis you have?\n",
      "Bot:  We have wide variety of puja related products and agarbattis.\n",
      "You: which scented agarbattis you have?\n",
      "Bot:  COD is not available for now, we accept only online payments.\n",
      "You: do you have masala agarbattis?\n",
      "Bot:  The costs of Wet Dhoop Sticks are as follows:Gugal-20pcs-35Rs,  Loban-20pcs-35Rs , Champa-20pcs-35Rs, Gulab-20pcs-35Rs, Mogra-20pcs-35Rs and Prachin-20pcs-35Rs. It will be packed in Zipper polybags.\n",
      "You: premium masala\n",
      "Bot:  We have Masala Agarbatti in different types like Mantra, Flora, Nag Champa, Nitya Puja, Masala Mogra, Masala Gulab, Indrayani Masala, Azaroo Masala, Natural Masala, Chandan Masala, Sai Flora Masala, Gold Flora, Chintamani Flora, Maratha Flora, Samrat Nag Champa, Loban Masala, Sai Chafa, White Gold, Azaroo and Natural.\n",
      "You: you have pouch?\n",
      "Bot:  Good to see you visit us.\n",
      "You: agarbatis in pouch\n",
      "Bot:  Hi there, how can I help?\n",
      "You: nag champa?\n",
      "Bot:  Masala Agarbattis will be box packed.\n",
      "You: flora\n",
      "Bot:  See you later, thanks for visiting.\n",
      "You: how will you deliver?\n",
      "Bot:  Hi there, how can I help?\n",
      "You: is your product packed?\n",
      "Bot:  The product will be box packed.\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [lemmatizer.lemmatize(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "        #print(results)\n",
    "        #print(tag)\n",
    "\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "            \n",
    "        print(\"Bot: \",random.choice(responses))\n",
    "\n",
    "chat()"
   ]
  }  
